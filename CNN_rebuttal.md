
# The answer of the reviewer 8pMB:

We thank the authors for their precise response to all of the points in our review.

The noted differences with prior work are appreciated, although it would be nice if the authors could extend this discussion significantly further in the main text.

The comments on baselines and evaluation criteria are also well received and appreciated.

We would like to clarify that our comment on the limited scalability of the model was mainly based on the symbolic categorical representation of the input. Therefore, if the authors have a way to use CNN based visual descriptors, and could be precise about how this would work in their manuscript, this would help significantly and avoid some of the scalability limitations.

Overall, if the authors make the changes noted in their response, the paper would improve significantly and I would be happy to raise my score.


The implmentation of the object detector in the paper is briefly explained as follows:

## Answer


[//]: # (To clarify, our system is intentionally designed with scalability and extensibility in mind, )

[//]: # (and the current symbolic representation is not a limitation of the architecture, )

[//]: # (but a design choice aligned with the current scope of the paper. )

[//]: # (The object detection component uses a feature memory bank approach, )

[//]: # (developed specifically to support robust shape recognition in a scalable and generalizable way.)


Thanks for the follow-up comment. For your **concerns on potential limitations imposed by relying on symbolic, categorical representations**, 
we appreciate this opportunity to clarify that the architecture of GRMs 
is **not limited to symbolic inputs** and has been **intentionally designed** to support more scalable 
and perceptually grounded representations through a memory-based feature matching approach.


The goal of this design is to support **robust shape recognition**, 
even in **challenging cases such as occlusion**. 
For example, even if a partially occluded triangle does not appear in the training set, 
our system can still recognize it at test time—as long as **enough of its local features are preserved**. 
This includes cases like the **classic Kanizsa triangle** (see Appendix C.4 `classic triangle`), 
where partial cues can be grouped into a complete percept.

To enable this, we extract **local visual features** (e.g., from keypoints like corners or edges) 
and store them in a **feature memory bank**. 
At test time, each object is matched against this bank using patch-level similarity, 
and a final label is assigned via **majority voting** over the matched features. 
This mechanism improves generalization across **appearance variation**, **partial visibility**, and **unseen compositions**.

While our current implementation focuses on geometric shapes, 
the same architecture can be extended to **naturalistic visual inputs** by improving the feature extractor—for instance, 
by using a more expressive CNN or transformer-based patch encoder. 
This would allow GRMs to build a **learned feature bank** of natural object parts (e.g., wheels, limbs, textures) and 
perform grouping and reasoning over these elements in the same way. 
Although this extension is outside the current paper's scope, 
we see it as a **natural and feasible direction** fully compatible with the proposed pipeline.

We will add this discussion to **Section 3.1** and the **Discussion section**, 
making it clear that our memory-based design is both **scalable and generalizable** by construction.



### Perceptor Dataset Preparation
We firstly generate a synthetic dataset of geometric shapes—triangles, squares, and circles—by creating grayscale images 
with randomly sized and positioned shapes. 
For each shape, it also records the key points (e.g. vertices or edge points) and extracts small image patches around them. 
These patches and keypoint data are saved alongside the full image in a structured format. 
The goal is to support training models that can detect and reason about shapes, even under challenging conditions like occlusion, 
by leveraging both visual and spatial information.

This dataset can be naturally extended to include other geometric or irregular shapes by adapting the image generation and keypoint extraction process. 
For example, polygons like pentagons or hexagons can be generated by placing vertices uniformly along a circle, 
while shapes like stars or ellipses can be handled by defining meaningful keypoints such as corners or boundary extremes. 
The core idea—capturing both the full shape and local patches around critical points—remains applicable, 
making the framework flexible for a wide variety of 2D shapes. 
Such extensions would enrich the dataset, enabling models to generalize better and reason about more complex or less regular structures.

### Perceptor Training

The main goal of training is to train a model (the perceptor) to recognize 
geometric shapes by learning local visual features around keypoints, such as corners or edges. 
Each shape is broken down into small patches centered at these points,
and the model learns to associate each patch with its geometric role (e.g., corner of a triangle).

To enhance robustness—especially when shapes are partially covered—
the code also extracts the shape’s contour and converts it into a sequence of direction vectors. 
These vectors represent how the shape flows from point to point, 
providing a global structural pattern.

By combining both local patch features and global contour directionality, 
the model is designed to infer the overall shape even when only parts of it are visible.

### Perceptor Applying

This implementation detects object shapes in test images by 
matching local visual features to a memory bank of learned patches. 
It first segments the image by identifying connected regions of the same color, 
treating each as an object. 
Each segment is then converted to a binarized grayscale image. 
The system compares patches from the object with stored patches 
from known shapes using a similarity threshold. 
If matches are found, the shape with the most frequent matching patches 
is selected as the object's label. 
This allows the model to recognize shapes even if they are partially visible.


##### Prompt 
The trained perceptor is used to detect the shape of the objects in the task images. 
Given a task image, it first detect the connected regions in the image. For each connected regions by one color, 
it is considered as an object in the image.
To detect the label of the object, we use the following code

```python

    """ recall the memory features from the given segments """
    seg_color_rgb = find_segment_color(segment)
    seg_color = bk.color_dict_rgb2name[tuple(seg_color_rgb)]
    # rgb segment to resized bw image

    # recall the memory
    # Define the grayscale conversion weights
    weights = torch.tensor([0.2989, 0.5870, 0.1140], device=segment.device)
    # Convert to grayscale using the dot product
    test_image = torch.tensordot(segment.float(), weights, dims=([-1], [0])).round().clamp(0, 255).to(
        torch.uint8)  # Shape: (1, H, W)

    # remove the gray background, set image to black-white image
    test_image[test_image == 211] = 0
    test_image[test_image > 0] = 1
    # Find similar patches
    dataset_patches = torch.cat([bk_shapes[i]["fm_repo"] for i in range(len(bk_shapes))], dim=0).to(args.device)
    dataset_patches[dataset_patches > 0] = 1
    vertex_labels = [bk_shapes[i]["labels"] for i in range(len(bk_shapes))]
    vertex_labels = torch.cat([vertex_labels[i] for i in range(len(vertex_labels))])
    shape_labels = [[i] * len(bk_shapes[i]["fm_repo"]) for i in range(len(bk_shapes))]
    shape_labels = list(itertools.chain.from_iterable(shape_labels))

    th = 0
    matches = find_similar_patches(test_image, dataset_patches, vertex_labels, shape_labels, patch_size=32,
                                   threshold=10)
    while len(matches) == 0:
        th += 1
        matches = find_similar_patches(test_image, dataset_patches, vertex_labels, shape_labels, patch_size=32,
                                       threshold=th)
    most_frequent_matches = extract_most_frequent_label_matches(matches)
    match_shape_id = most_frequent_matches[0]["shape_label"] + 1

```




##### Prompt

During the training, the goal of the perceptor is to learn the local features of the objects.
It is designed to be robusted to the test dataset even if the objects are partially covered, 
it should still recall the object label by observing the enough hinted partial features of the test objects.
The code for this part is shown as follows: 
```python

def contour_to_direction_vector(contour):
    """
    Convert a contour into a direction vector.

    Args:
        contour (np.ndarray): Contour represented as an array of points of shape (N, 1, 2).

    Returns:
        list: A list of tuples representing direction vectors (dx, dy) for each point in the contour.
    """
    # Extract the points from the contour
    points = contour  # Shape becomes (N, 2)

    # Compute the direction vector
    direction_vector = []
    num_points = len(points)
    for i in range(num_points):
        # Compute the difference between consecutive points
        dx = points[(i + 1) % num_points][0] - points[i][0]  # Next point wraps around
        dy = points[(i + 1) % num_points][1] - points[i][1]
        direction_vector.append((dx, dy))
    angles = direction_vectors_to_angles(np.array(direction_vector))
    smoothed_angles = smooth_directions_degrees(angles, window_size=10, sharpness_threshold=80)

    return smoothed_angles

def identify_fms(shape):
    # Load metadata
    metadata_path = config.kp_base_dataset / shape / "metadata.json"
    if not os.path.exists(str(metadata_path)):
        raise FileNotFoundError(f"Metadata file not found in {str(metadata_path)}")
    metadata = data_utils.load_json(metadata_path)
    patches = []
    labels = []
    patch_set = set()
    # Iterate through the metadata
    for entry in metadata:
        for p_i, patch_filename in enumerate(entry["patches"]):
            patch_path = config.kp_base_dataset / shape / patch_filename
            if os.path.exists(patch_path):
                patch_image = Image.open(patch_path)
                patch_array = np.array(patch_image)
                patch_tuple = tuple(map(tuple, patch_array))
                if patch_tuple not in patch_set:
                    patch_set.add(patch_tuple)
                    patches.append(torch.from_numpy(patch_array))
                    labels.append(p_i)

    fm_all = torch.stack(patches)
    img_path = config.kp_base_dataset / shape / metadata[0]['image']
    image = np.array(Image.open(img_path))
    # get the contours
    contour_points = data_utils.get_contours(image)
    contour_points = contour_points.reshape(-1, 2)
    contour_img = np.zeros((512, 512, 3), dtype=np.uint8)
    from src import bk
    for i in range(len(contour_points)):
        pos = contour_points[i]
        if i < 5:
            contour_img[pos[1], pos[0]] = [255, 0, 0]
        else:
            contour_img[pos[1], pos[0]] = [255, 255, 255]
    van(contour_img)
    direction_vector = torch.tensor(data_utils.contour_to_direction_vector(contour_points))
    return fm_all, direction_vector, labels


```


### 

``` 

def generate_circle_image(image_size=512, min_size=20, max_size=200, hollow=False, max_border_width=10):
    """
    Generate a single image with a random circle.

    Args:
        image_size (int): Size of the square image (image_size x image_size).
        min_size (int): Minimum size of the circle diameter.
        max_size (int): Maximum size of the circle diameter.
        hollow (bool): Whether the circle should be hollow (transparent inside).
        max_border_width (int): Maximum width of the border for hollow circles.

    Returns:
        np.ndarray: Generated image as a NumPy array.
        list: Center and radius of the circle.
    """
    # Create a blank transparent image
    image = Image.new("L", (image_size, image_size), color="black")
    draw = ImageDraw.Draw(image)

    # Randomly determine the circle diameter
    circle_diameter = np.random.randint(min_size, max_size)

    # Ensure the circle is fully within bounds
    margin = 50 + circle_diameter // 2
    x_center = np.random.randint(margin, image_size - margin)
    y_center = np.random.randint(margin, image_size - margin)

    # Define the bounding box for the circle
    bounding_box = [
        (x_center - circle_diameter // 2, y_center - circle_diameter // 2),
        (x_center + circle_diameter // 2, y_center + circle_diameter // 2)
    ]

    # Random color for the circle
    color = 255

    # Draw the filled circle
    draw.ellipse(bounding_box, fill=color)
    # Calculate four random points on the edge of the circle
    angles = np.random.uniform(0, 2 * np.pi, 4)
    edge_points = [
        (x_center + int((circle_diameter // 2) * np.cos(angle)),
         y_center + int((circle_diameter // 2) * np.sin(angle)))
        for angle in angles
    ]
    return np.array(image), edge_points
    
def generate_square_image(image_size=512, min_size=20, max_size=200, hollow=False, max_border_width=10):
    """
    Generate a single image with a random square.

    Args:
        image_size (int): Size of the square image (image_size x image_size).
        min_size (int): Minimum size of the square.
        max_size (int): Maximum size of the square.
        hollow (bool): Whether the square should be hollow (transparent inside).
        max_border_width (int): Maximum width of the border for hollow squares.

    Returns:
        np.ndarray: Generated image as a NumPy array.
        list: List of vertices positions.
    """
    # Create a blank transparent image
    image = Image.new("L", (image_size, image_size), color="black")
    draw = ImageDraw.Draw(image)

    # Randomly determine the square size
    square_size = np.random.randint(min_size, max_size)

    # Ensure the square is fully within bounds
    margin = 50 + square_size // 2
    x_center = np.random.randint(margin, image_size - margin)
    y_center = np.random.randint(margin, image_size - margin)

    # Calculate the vertices of the square
    vertices = [
        (x_center - square_size // 2, y_center - square_size // 2),  # Top-left vertex
        (x_center + square_size // 2, y_center - square_size // 2),  # Top-right vertex
        (x_center + square_size // 2, y_center + square_size // 2),  # Bottom-right vertex
        (x_center - square_size // 2, y_center + square_size // 2)  # Bottom-left vertex
    ]

    # Random color for the square
    color = 255

    draw.rectangle((vertices[0], vertices[2]), fill=color)

    return np.array(image), vertices

def generate_triangle_image(image_size=512, min_size=20, max_size=200):
    """
    Generate a single image with a random triangle.

    Args:
        image_size (int): Size of the square image (image_size x image_size).
        min_size (int): Minimum size of the triangle.
        max_size (int): Maximum size of the triangle.

    Returns:
        np.ndarray: Generated image as a NumPy array.
        list: List of vertices positions.
    """
    # Create a blank white image
    image = Image.new("L", (image_size, image_size), color="black")
    draw = ImageDraw.Draw(image)

    # Randomly determine the triangle size
    triangle_size = np.random.randint(min_size, max_size)

    # Ensure the triangle is fully within bounds
    margin = 50 + triangle_size // 2
    x_center = np.random.randint(margin, image_size - margin)
    y_center = np.random.randint(margin, image_size - margin)

    # Calculate the vertices of the triangle
    half_height = int(np.sqrt(3) / 2 * triangle_size)
    vertices = [
        (x_center, y_center - 2 * half_height // 3),  # Top vertex
        (x_center - triangle_size // 2, y_center + half_height // 3),  # Bottom-left vertex
        (x_center + triangle_size // 2, y_center + half_height // 3)  # Bottom-right vertex
    ]

    # Random color for the triangle
    color = 255

    # Draw the triangle
    draw.polygon(vertices, fill=color)

    return np.array(image), vertices
    
def extract_patches(image, vertices, patch_size=32):
    """
    Extract patches of size patch_size x patch_size around each vertex.

    Args:
        image (np.ndarray): Input image as a NumPy array.
        vertices (list): List of (x, y) tuples representing vertex coordinates.
        patch_size (int): Size of the patch to extract (default is 32).

    Returns:
        list: List of extracted patches as NumPy arrays.
    """
    patches = []
    half_size = patch_size // 2
    padded_image = np.pad(image, ((half_size, half_size), (half_size, half_size)), mode='constant', constant_values=0)

    for (x, y) in vertices:
        x_p, y_p = x + half_size, y + half_size
        patch = padded_image[y_p - half_size:y_p + half_size, x_p - half_size:x_p + half_size]
        patches.append(patch)

    return patches
    
    
def generate_dataset(shape, output_dir="triangle_dataset", num_images=1000, image_size=512):
    """
    Generate a dataset of triangle images.

    Args:
        output_dir (str): Directory to save the dataset.
        num_images (int): Number of images to generate.
        image_size (int): Size of the square image (image_size x image_size).
    """
    os.makedirs(output_dir, exist_ok=True)
    metadata = []

    for i in tqdm(range(num_images)):
        # Generate a triangle image
        if shape == "triangle":
            image, vertices = generate_triangle_image(image_size=image_size)
        elif shape == "square":
            image, vertices = generate_square_image(image_size=image_size)
        elif shape == "circle":
            image, vertices = generate_circle_image(image_size=image_size)
        else:
            raise ValueError

        # Extract patches for each vertex
        patches = extract_patches(image, vertices)

        # Save the image
        image_filename = os.path.join(output_dir, f"triangle_{i:04d}.png")
        image = Image.fromarray(image)
        image.save(image_filename)

        # Save patches
        for j, patch in enumerate(patches):
            patch_filename = os.path.join(output_dir, f"{shape}_{i:04d}_patch_{j}.png")
            patch_image = Image.fromarray(patch)
            patch_image.save(patch_filename)

        # Record metadata
        metadata.append({
            "image": f"triangle_{i:04d}.png",
            "vertices": vertices,
            "patches": [f"{shape}_{i:04d}_patch_{j}.png" for j in range(len(patches))]
        })

        # if (i + 1) % 100 == 0:
        #     print(f"Generated {i + 1}/{num_images} images")

    # Save metadata as JSON
    metadata_filename = os.path.join(output_dir, f"metadata.json")
    with open(metadata_filename, "w") as f:
        json.dump(metadata, f, indent=4)

    print(f"Metadata saved to '{metadata_filename}'.")

```